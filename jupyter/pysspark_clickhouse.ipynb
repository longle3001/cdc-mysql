{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File \"data.json\" uploaded successfully to bucket \"mybucket\".\n",
      "Objects in mybucket:\n",
      "  stripe/2024/07/15/type=json/users_153357.json\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import boto3 \n",
    "from botocore.client import Config\n",
    "\n",
    "localstack_url = 'http://localhost:4566'\n",
    "\n",
    "data = [\n",
    "    {\"name\": \"Alice\", \"age\": 25, \"email\": \"alice@gmail.com\"},\n",
    "    {\"name\": \"Bob\", \"age\": 30, \"email\": \"bob@gmail.com\"},\n",
    "    {\"name\": \"Cathy\", \"age\": 22, \"email\": \"cathy@gmail.com\"}\n",
    "]\n",
    "\n",
    "\n",
    "def store_s3_json(s3_client, items: list, bucket_name: str):\n",
    "    try:\n",
    "        current_date = datetime.now().strftime(\"%Y/%m/%d\")\n",
    "        prefix = f\"stripe/{current_date}/type=json\"\n",
    "        file_name = f\"{prefix}/users_{datetime.now().strftime('%H%M%S')}.json\"\n",
    "        json_data = json.dumps(items)\n",
    "        s3_client.put_object(Bucket=bucket_name, Key=file_name, Body=json_data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "# Kết nối tới MinIO\n",
    "# s3 = boto3.client(\n",
    "#     's3',\n",
    "#     endpoint_url='http://localhost:9001',\n",
    "#     aws_access_key_id='minio',\n",
    "#     aws_secret_access_key='minio123',\n",
    "#     config=Config(signature_version='s3v4')\n",
    "# )\n",
    "\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url=localstack_url,\n",
    "    aws_access_key_id='test',  # Use any access key and secret key for LocalStack\n",
    "    aws_secret_access_key='test',\n",
    "    config=Config(signature_version='s3v4')\n",
    ")\n",
    "\n",
    "# s3.create_bucket(Bucket='mybucket')\n",
    "\n",
    "# Tên bucket\n",
    "bucket_name = 'mybucket'\n",
    "\n",
    "# Tải file lên MinIO\n",
    "file_name = 'data.json'\n",
    "store_s3_json(s3, items=data, bucket_name=\"mybucket\")\n",
    "print(f'File \"{file_name}\" uploaded successfully to bucket \"{bucket_name}\".')\n",
    "\n",
    "\n",
    "# List objects in the bucket\n",
    "response = s3.list_objects(Bucket='mybucket')\n",
    "print('Objects in mybucket:')\n",
    "for obj in response.get('Contents', []):\n",
    "    print(f'  {obj[\"Key\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPARK_HOME: /opt/homebrew/Cellar/apache-spark/3.5.1/libexec\n",
      "AWS_ACCESS_KEY_ID: dummy\n",
      "AWS_SECRET_ACCESS_KEY: dummy\n",
      ":: loading settings :: url = jar:file:/opt/homebrew/Cellar/apache-spark/3.5.1/libexec/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/longle/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/longle/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      "ru.yandex.clickhouse#clickhouse-jdbc added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-52c49acd-eeb5-4bb6-9971-39703fbca053;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.2 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.1026 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound ru.yandex.clickhouse#clickhouse-jdbc;0.2.6 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.13 in central\n",
      "\tfound commons-logging#commons-logging;1.2 in central\n",
      "\tfound commons-codec#commons-codec;1.11 in central\n",
      "\tfound org.apache.httpcomponents#httpmime;4.5.13 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.9.10 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.9.10.8 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.9.10 in central\n",
      "\tfound com.google.guava#guava;29.0-jre in central\n",
      "\tfound com.google.guava#failureaccess;1.0.1 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound org.checkerframework#checker-qual;2.11.1 in central\n",
      "\tfound com.google.errorprone#error_prone_annotations;2.3.4 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.3 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound javax.xml.bind#jaxb-api;2.3.1 in central\n",
      "\tfound javax.activation#javax.activation-api;1.2.0 in central\n",
      ":: resolution report :: resolve 1496ms :: artifacts dl 51ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.1026 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.9.10 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.9.10 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.9.10.8 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.errorprone#error_prone_annotations;2.3.4 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0.1 from central in [default]\n",
      "\tcom.google.guava#guava;29.0-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.3 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.11 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.2 from central in [default]\n",
      "\tjavax.activation#javax.activation-api;1.2.0 from central in [default]\n",
      "\tjavax.xml.bind#jaxb-api;2.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.2 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpmime;4.5.13 from central in [default]\n",
      "\torg.checkerframework#checker-qual;2.11.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\tru.yandex.clickhouse#clickhouse-jdbc;0.2.6 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.375 by [com.amazonaws#aws-java-sdk-bundle;1.11.1026] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   24  |   0   |   0   |   1   ||   23  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-52c49acd-eeb5-4bb6-9971-39703fbca053\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 23 already retrieved (0kB/33ms)\n",
      "24/07/15 15:34:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/07/15 15:34:28 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----+\n",
      "|age|               email| name|\n",
      "+---+--------------------+-----+\n",
      "| 25|0129c7f7c8cd4dda3...|Alice|\n",
      "| 30|df6b0a875d719135e...|  Bob|\n",
      "| 22|1676e9f3b5a8004cd...|Cathy|\n",
      "+---+--------------------+-----+\n",
      "\n",
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# # Set environment variables\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/homebrew/Cellar/apache-spark/3.5.1/libexec\"\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"dummy\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"dummy\"\n",
    "\n",
    "# Verify environment variables\n",
    "print(\"SPARK_HOME:\", os.environ[\"SPARK_HOME\"])\n",
    "print(\"AWS_ACCESS_KEY_ID:\", os.environ.get(\"AWS_ACCESS_KEY_ID\"))\n",
    "print(\"AWS_SECRET_ACCESS_KEY:\", os.environ.get(\"AWS_SECRET_ACCESS_KEY\"))\n",
    "\n",
    "\n",
    "import hashlib\n",
    "\n",
    "def pseudonymize_doc_string(doc):\n",
    "    '''\n",
    "    Pseudonmyisation is a deterministic type of PII-obscuring\n",
    "    Its role is to allow identifying users by their hash,\n",
    "    without revealing the underlying info.\n",
    "    '''\n",
    "    # add a constant salt to generate\n",
    "    salt = 'WI@N57%zZrmk#88c'\n",
    "    salted_string = doc + salt\n",
    "    sh = hashlib.sha256()\n",
    "    sh.update(salted_string.encode())\n",
    "    hashed_string = sh.digest().hex()\n",
    "    return hashed_string\n",
    "\n",
    "\n",
    "import findspark\n",
    "# Initialize findspark with the specified SPARK_HOME\n",
    "findspark.init(os.environ[\"SPARK_HOME\"])\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"S3App\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.2,com.amazonaws:aws-java-sdk-bundle:1.11.375,ru.yandex.clickhouse:clickhouse-jdbc:0.2.6\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", os.environ[\"AWS_ACCESS_KEY_ID\"]) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", os.environ[\"AWS_SECRET_ACCESS_KEY\"]) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://localhost:4566\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "pseudonymize_udf = F.udf(pseudonymize_doc_string, StringType())\n",
    "\n",
    "# Read a JSON file from S3 (LocalStack)\n",
    "try:\n",
    "    df = spark.read.json(\"s3a://mybucket/stripe/2024/07/15/type=json/users_153357.json\")\n",
    "    df = df.withColumn(\"email\", pseudonymize_udf(F.col(\"email\")))\n",
    "    df.show()\n",
    "    df.printSchema()\n",
    "except Exception as e:\n",
    "    print(f\"Error reading data: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written to ClickHouse successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/15 15:34:46 WARN JdbcUtils: Requested isolation level 1, but transactions are unsupported\n"
     ]
    }
   ],
   "source": [
    "\n",
    "clickhouse_url = \"jdbc:clickhouse://localhost:8123/default\"\n",
    "clickhouse_properties = {\n",
    "    \"driver\": \"ru.yandex.clickhouse.ClickHouseDriver\",\n",
    "    \"user\": \"clickhouse-user\",\n",
    "    \"password\": \"secret\"\n",
    "}\n",
    "\n",
    "# Write DataFrame to ClickHouse\n",
    "try:\n",
    "    df.write \\\n",
    "      .format(\"jdbc\") \\\n",
    "      .option(\"url\", clickhouse_url) \\\n",
    "      .option(\"dbtable\", \"users\") \\\n",
    "      .option(\"user\", clickhouse_properties[\"user\"]) \\\n",
    "      .option(\"password\", clickhouse_properties[\"password\"]) \\\n",
    "      .option(\"driver\", clickhouse_properties[\"driver\"]) \\\n",
    "      .mode(\"append\") \\\n",
    "      .save()\n",
    "    print(\"Data written to ClickHouse successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error writing data to ClickHouse: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted IDs: [ObjectId('66953c538283b50447c427db'), ObjectId('66953c538283b50447c427dc'), ObjectId('66953c538283b50447c427dd')]\n",
      "{'_id': ObjectId('6694de078283b50447c427d7'), 'email': 'user1@example.com', 'name': 'User One', 'age': 25}\n",
      "{'_id': ObjectId('6694de078283b50447c427d8'), 'email': 'user2@example.com', 'name': 'User Two', 'age': 30}\n",
      "{'_id': ObjectId('6694de078283b50447c427d9'), 'email': 'user3@example.com', 'name': 'User Three', 'age': 22}\n",
      "{'_id': ObjectId('66953c538283b50447c427db'), 'email': 'user11@example.com', 'name': 'User One 1', 'age': 25}\n",
      "{'_id': ObjectId('66953c538283b50447c427dc'), 'email': 'user21@example.com', 'name': 'User Two 1', 'age': 30}\n",
      "{'_id': ObjectId('66953c538283b50447c427dd'), 'email': 'user31@example.com', 'name': 'User Three 1', 'age': 22}\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "# Cấu hình thông tin xác thực\n",
    "username = 'admin' # Người dùng bạn đã tạo trong mongo-init.js hoặc MONGO_INITDB_ROOT_USERNAME\n",
    "password = 'admin' # Mật khẩu tương ứng\n",
    "host = 'localhost' # Hoặc tên dịch vụ MongoDB trong Docker Compose nếu Python chạy trong cùng môi trường Docker\n",
    "port = '27017'\n",
    "database_name = 'mongo'\n",
    "\n",
    "# Tạo chuỗi kết nối\n",
    "connection_string = f'mongodb://{username}:{password}@{host}:{port}/{database_name}'\n",
    "# connection_string = f'mongodb://{username}:{password}@{host}:{port}/{database_name}?replicaSet=rs0'\n",
    "\n",
    "# Kết nối tới MongoDB\n",
    "client = MongoClient(connection_string)\n",
    "db = client[database_name]\n",
    "\n",
    "\n",
    "# Access the collection\n",
    "collection = db.mongousers\n",
    "\n",
    "# Sample data to insert\n",
    "data = [\n",
    "    {\"_id\":1, \"email\": \"user11@example.com\", \"name\": \"User One 1\", \"age\": 25},\n",
    "    {\"_id\":2, \"email\": \"user21@example.com\", \"name\": \"User Two 1\", \"age\": 30},\n",
    "    {\"_id\":3,\"email\": \"user31@example.com\", \"name\": \"User Three 1\", \"age\": 22}\n",
    "]\n",
    "\n",
    "# Insert data into the collection\n",
    "result = collection.insert_many(data)\n",
    "\n",
    "# Print the inserted IDs\n",
    "print(\"Inserted IDs:\", result.inserted_ids)\n",
    "\n",
    "documents = collection.find({}) \n",
    "for doc in documents:\n",
    "    print(doc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
